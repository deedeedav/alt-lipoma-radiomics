{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f07c390f",
   "metadata": {},
   "source": [
    "# Nested cross-validation structure with feature selection using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "059eef78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import scipy\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectPercentile, mutual_info_classif\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from scipy import interp\n",
    "from statistics import stdev\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.transforms as mtransforms\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "import itertools\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22e81a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/harmonized/combined_harmonized.csv\", sep=\"\\t\")\n",
    "\n",
    "df2 = pd.read_csv(\"../../data/clinical/clinical_data_internal_combined.csv\", sep=\";\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd885124",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.drop(df2.columns[1:6], axis=1, inplace=True)\n",
    "df2.drop(df2.columns[2], axis=1, inplace=True)\n",
    "df = df2.join(df.set_index('ID_intern'), on='ID_intern')\n",
    "df = df.rename({'Pathology': 'Type'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a93dd46e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_intern</th>\n",
       "      <th>Type</th>\n",
       "      <th>T1_original_shape_Elongation</th>\n",
       "      <th>T1_original_shape_Flatness</th>\n",
       "      <th>T1_original_shape_LeastAxisLength</th>\n",
       "      <th>T1_original_shape_MajorAxisLength</th>\n",
       "      <th>T1_original_shape_Maximum2DDiameterColumn</th>\n",
       "      <th>T1_original_shape_Maximum2DDiameterRow</th>\n",
       "      <th>T1_original_shape_Maximum2DDiameterSlice</th>\n",
       "      <th>T1_original_shape_Maximum3DDiameter</th>\n",
       "      <th>...</th>\n",
       "      <th>T1fs_original_gldm_DependenceVariance</th>\n",
       "      <th>T1fs_original_gldm_GrayLevelNonUniformity</th>\n",
       "      <th>T1fs_original_gldm_GrayLevelVariance</th>\n",
       "      <th>T1fs_original_gldm_HighGrayLevelEmphasis</th>\n",
       "      <th>T1fs_original_gldm_LargeDependenceEmphasis</th>\n",
       "      <th>T1fs_original_gldm_LargeDependenceLowGrayLevelEmphasis</th>\n",
       "      <th>T1fs_original_gldm_LowGrayLevelEmphasis</th>\n",
       "      <th>T1fs_original_gldm_SmallDependenceEmphasis</th>\n",
       "      <th>T1fs_original_gldm_SmallDependenceHighGrayLevelEmphasis</th>\n",
       "      <th>T1fs_original_gldm_SmallDependenceLowGrayLevelEmphasis</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LIPO_002</td>\n",
       "      <td>0</td>\n",
       "      <td>0.250806</td>\n",
       "      <td>0.139088</td>\n",
       "      <td>0.225144</td>\n",
       "      <td>0.486140</td>\n",
       "      <td>0.189037</td>\n",
       "      <td>0.336436</td>\n",
       "      <td>0.463397</td>\n",
       "      <td>0.447998</td>\n",
       "      <td>...</td>\n",
       "      <td>0.392114</td>\n",
       "      <td>0.037301</td>\n",
       "      <td>0.021410</td>\n",
       "      <td>0.059376</td>\n",
       "      <td>0.217081</td>\n",
       "      <td>0.029379</td>\n",
       "      <td>0.069781</td>\n",
       "      <td>0.051594</td>\n",
       "      <td>0.001241</td>\n",
       "      <td>0.019483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LIPO_006</td>\n",
       "      <td>0</td>\n",
       "      <td>0.956647</td>\n",
       "      <td>0.839193</td>\n",
       "      <td>0.903002</td>\n",
       "      <td>0.535274</td>\n",
       "      <td>0.619874</td>\n",
       "      <td>0.565238</td>\n",
       "      <td>0.570082</td>\n",
       "      <td>0.563262</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116511</td>\n",
       "      <td>0.142997</td>\n",
       "      <td>0.075332</td>\n",
       "      <td>0.159306</td>\n",
       "      <td>0.058250</td>\n",
       "      <td>-0.002527</td>\n",
       "      <td>0.025307</td>\n",
       "      <td>0.171700</td>\n",
       "      <td>0.050888</td>\n",
       "      <td>0.030947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LIPO_007</td>\n",
       "      <td>1</td>\n",
       "      <td>0.486505</td>\n",
       "      <td>0.467856</td>\n",
       "      <td>0.291966</td>\n",
       "      <td>0.276583</td>\n",
       "      <td>0.175388</td>\n",
       "      <td>0.272151</td>\n",
       "      <td>0.268000</td>\n",
       "      <td>0.259158</td>\n",
       "      <td>...</td>\n",
       "      <td>0.290507</td>\n",
       "      <td>0.022492</td>\n",
       "      <td>0.073235</td>\n",
       "      <td>0.056163</td>\n",
       "      <td>0.117083</td>\n",
       "      <td>0.030527</td>\n",
       "      <td>0.107069</td>\n",
       "      <td>0.156891</td>\n",
       "      <td>0.031479</td>\n",
       "      <td>0.062275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LIPO_009</td>\n",
       "      <td>0</td>\n",
       "      <td>0.865762</td>\n",
       "      <td>0.639992</td>\n",
       "      <td>0.944379</td>\n",
       "      <td>0.710976</td>\n",
       "      <td>0.708308</td>\n",
       "      <td>0.661475</td>\n",
       "      <td>0.680350</td>\n",
       "      <td>0.697248</td>\n",
       "      <td>...</td>\n",
       "      <td>0.312078</td>\n",
       "      <td>0.219517</td>\n",
       "      <td>0.104173</td>\n",
       "      <td>0.196247</td>\n",
       "      <td>0.137075</td>\n",
       "      <td>0.001853</td>\n",
       "      <td>0.016859</td>\n",
       "      <td>0.120955</td>\n",
       "      <td>0.069555</td>\n",
       "      <td>0.003478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LIPO_010</td>\n",
       "      <td>1</td>\n",
       "      <td>0.501202</td>\n",
       "      <td>0.383670</td>\n",
       "      <td>0.370937</td>\n",
       "      <td>0.414304</td>\n",
       "      <td>0.417894</td>\n",
       "      <td>0.402481</td>\n",
       "      <td>0.210499</td>\n",
       "      <td>0.388404</td>\n",
       "      <td>...</td>\n",
       "      <td>0.580909</td>\n",
       "      <td>0.122396</td>\n",
       "      <td>0.010837</td>\n",
       "      <td>0.037791</td>\n",
       "      <td>0.334436</td>\n",
       "      <td>0.078517</td>\n",
       "      <td>0.110207</td>\n",
       "      <td>0.039649</td>\n",
       "      <td>0.000776</td>\n",
       "      <td>0.014309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>LT202</td>\n",
       "      <td>0</td>\n",
       "      <td>0.610670</td>\n",
       "      <td>0.148018</td>\n",
       "      <td>0.067588</td>\n",
       "      <td>0.162248</td>\n",
       "      <td>0.103619</td>\n",
       "      <td>0.162699</td>\n",
       "      <td>0.162354</td>\n",
       "      <td>0.152985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.767144</td>\n",
       "      <td>0.004832</td>\n",
       "      <td>0.012005</td>\n",
       "      <td>0.027382</td>\n",
       "      <td>0.415566</td>\n",
       "      <td>0.151211</td>\n",
       "      <td>0.197115</td>\n",
       "      <td>0.057972</td>\n",
       "      <td>0.004302</td>\n",
       "      <td>0.066838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>LT203</td>\n",
       "      <td>0</td>\n",
       "      <td>0.758654</td>\n",
       "      <td>0.267662</td>\n",
       "      <td>0.083371</td>\n",
       "      <td>0.130821</td>\n",
       "      <td>0.123474</td>\n",
       "      <td>0.126999</td>\n",
       "      <td>0.111748</td>\n",
       "      <td>0.125729</td>\n",
       "      <td>...</td>\n",
       "      <td>0.295447</td>\n",
       "      <td>0.012926</td>\n",
       "      <td>0.023644</td>\n",
       "      <td>0.039880</td>\n",
       "      <td>0.149361</td>\n",
       "      <td>0.053821</td>\n",
       "      <td>0.157847</td>\n",
       "      <td>0.136706</td>\n",
       "      <td>0.021127</td>\n",
       "      <td>0.127816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>LT208</td>\n",
       "      <td>1</td>\n",
       "      <td>0.261232</td>\n",
       "      <td>0.376160</td>\n",
       "      <td>0.228461</td>\n",
       "      <td>0.269606</td>\n",
       "      <td>0.132136</td>\n",
       "      <td>0.188281</td>\n",
       "      <td>0.251662</td>\n",
       "      <td>0.251105</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109881</td>\n",
       "      <td>0.014791</td>\n",
       "      <td>0.138150</td>\n",
       "      <td>0.272585</td>\n",
       "      <td>0.065258</td>\n",
       "      <td>0.010759</td>\n",
       "      <td>0.031444</td>\n",
       "      <td>0.206859</td>\n",
       "      <td>0.096794</td>\n",
       "      <td>0.041305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>LT214</td>\n",
       "      <td>0</td>\n",
       "      <td>0.716600</td>\n",
       "      <td>0.580943</td>\n",
       "      <td>0.286281</td>\n",
       "      <td>0.240362</td>\n",
       "      <td>0.234626</td>\n",
       "      <td>0.204441</td>\n",
       "      <td>0.251003</td>\n",
       "      <td>0.248412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.295811</td>\n",
       "      <td>0.044483</td>\n",
       "      <td>0.031887</td>\n",
       "      <td>0.043628</td>\n",
       "      <td>0.170653</td>\n",
       "      <td>0.068206</td>\n",
       "      <td>0.178974</td>\n",
       "      <td>0.109040</td>\n",
       "      <td>0.024246</td>\n",
       "      <td>0.098005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>LT215</td>\n",
       "      <td>0</td>\n",
       "      <td>0.288325</td>\n",
       "      <td>0.281362</td>\n",
       "      <td>0.183458</td>\n",
       "      <td>0.270948</td>\n",
       "      <td>0.245214</td>\n",
       "      <td>0.230330</td>\n",
       "      <td>0.105382</td>\n",
       "      <td>0.223918</td>\n",
       "      <td>...</td>\n",
       "      <td>0.594670</td>\n",
       "      <td>0.023840</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.035353</td>\n",
       "      <td>0.414957</td>\n",
       "      <td>0.155286</td>\n",
       "      <td>0.182980</td>\n",
       "      <td>0.038477</td>\n",
       "      <td>0.010731</td>\n",
       "      <td>0.034973</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 290 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID_intern  Type  T1_original_shape_Elongation  T1_original_shape_Flatness  \\\n",
       "ID                                                                              \n",
       "0    LIPO_002     0                      0.250806                    0.139088   \n",
       "1    LIPO_006     0                      0.956647                    0.839193   \n",
       "2    LIPO_007     1                      0.486505                    0.467856   \n",
       "3    LIPO_009     0                      0.865762                    0.639992   \n",
       "4    LIPO_010     1                      0.501202                    0.383670   \n",
       "..        ...   ...                           ...                         ...   \n",
       "150     LT202     0                      0.610670                    0.148018   \n",
       "151     LT203     0                      0.758654                    0.267662   \n",
       "152     LT208     1                      0.261232                    0.376160   \n",
       "153     LT214     0                      0.716600                    0.580943   \n",
       "154     LT215     0                      0.288325                    0.281362   \n",
       "\n",
       "     T1_original_shape_LeastAxisLength  T1_original_shape_MajorAxisLength  \\\n",
       "ID                                                                          \n",
       "0                             0.225144                           0.486140   \n",
       "1                             0.903002                           0.535274   \n",
       "2                             0.291966                           0.276583   \n",
       "3                             0.944379                           0.710976   \n",
       "4                             0.370937                           0.414304   \n",
       "..                                 ...                                ...   \n",
       "150                           0.067588                           0.162248   \n",
       "151                           0.083371                           0.130821   \n",
       "152                           0.228461                           0.269606   \n",
       "153                           0.286281                           0.240362   \n",
       "154                           0.183458                           0.270948   \n",
       "\n",
       "     T1_original_shape_Maximum2DDiameterColumn  \\\n",
       "ID                                               \n",
       "0                                     0.189037   \n",
       "1                                     0.619874   \n",
       "2                                     0.175388   \n",
       "3                                     0.708308   \n",
       "4                                     0.417894   \n",
       "..                                         ...   \n",
       "150                                   0.103619   \n",
       "151                                   0.123474   \n",
       "152                                   0.132136   \n",
       "153                                   0.234626   \n",
       "154                                   0.245214   \n",
       "\n",
       "     T1_original_shape_Maximum2DDiameterRow  \\\n",
       "ID                                            \n",
       "0                                  0.336436   \n",
       "1                                  0.565238   \n",
       "2                                  0.272151   \n",
       "3                                  0.661475   \n",
       "4                                  0.402481   \n",
       "..                                      ...   \n",
       "150                                0.162699   \n",
       "151                                0.126999   \n",
       "152                                0.188281   \n",
       "153                                0.204441   \n",
       "154                                0.230330   \n",
       "\n",
       "     T1_original_shape_Maximum2DDiameterSlice  \\\n",
       "ID                                              \n",
       "0                                    0.463397   \n",
       "1                                    0.570082   \n",
       "2                                    0.268000   \n",
       "3                                    0.680350   \n",
       "4                                    0.210499   \n",
       "..                                        ...   \n",
       "150                                  0.162354   \n",
       "151                                  0.111748   \n",
       "152                                  0.251662   \n",
       "153                                  0.251003   \n",
       "154                                  0.105382   \n",
       "\n",
       "     T1_original_shape_Maximum3DDiameter  ...  \\\n",
       "ID                                        ...   \n",
       "0                               0.447998  ...   \n",
       "1                               0.563262  ...   \n",
       "2                               0.259158  ...   \n",
       "3                               0.697248  ...   \n",
       "4                               0.388404  ...   \n",
       "..                                   ...  ...   \n",
       "150                             0.152985  ...   \n",
       "151                             0.125729  ...   \n",
       "152                             0.251105  ...   \n",
       "153                             0.248412  ...   \n",
       "154                             0.223918  ...   \n",
       "\n",
       "     T1fs_original_gldm_DependenceVariance  \\\n",
       "ID                                           \n",
       "0                                 0.392114   \n",
       "1                                 0.116511   \n",
       "2                                 0.290507   \n",
       "3                                 0.312078   \n",
       "4                                 0.580909   \n",
       "..                                     ...   \n",
       "150                               0.767144   \n",
       "151                               0.295447   \n",
       "152                               0.109881   \n",
       "153                               0.295811   \n",
       "154                               0.594670   \n",
       "\n",
       "     T1fs_original_gldm_GrayLevelNonUniformity  \\\n",
       "ID                                               \n",
       "0                                     0.037301   \n",
       "1                                     0.142997   \n",
       "2                                     0.022492   \n",
       "3                                     0.219517   \n",
       "4                                     0.122396   \n",
       "..                                         ...   \n",
       "150                                   0.004832   \n",
       "151                                   0.012926   \n",
       "152                                   0.014791   \n",
       "153                                   0.044483   \n",
       "154                                   0.023840   \n",
       "\n",
       "     T1fs_original_gldm_GrayLevelVariance  \\\n",
       "ID                                          \n",
       "0                                0.021410   \n",
       "1                                0.075332   \n",
       "2                                0.073235   \n",
       "3                                0.104173   \n",
       "4                                0.010837   \n",
       "..                                    ...   \n",
       "150                              0.012005   \n",
       "151                              0.023644   \n",
       "152                              0.138150   \n",
       "153                              0.031887   \n",
       "154                              0.020738   \n",
       "\n",
       "     T1fs_original_gldm_HighGrayLevelEmphasis  \\\n",
       "ID                                              \n",
       "0                                    0.059376   \n",
       "1                                    0.159306   \n",
       "2                                    0.056163   \n",
       "3                                    0.196247   \n",
       "4                                    0.037791   \n",
       "..                                        ...   \n",
       "150                                  0.027382   \n",
       "151                                  0.039880   \n",
       "152                                  0.272585   \n",
       "153                                  0.043628   \n",
       "154                                  0.035353   \n",
       "\n",
       "     T1fs_original_gldm_LargeDependenceEmphasis  \\\n",
       "ID                                                \n",
       "0                                      0.217081   \n",
       "1                                      0.058250   \n",
       "2                                      0.117083   \n",
       "3                                      0.137075   \n",
       "4                                      0.334436   \n",
       "..                                          ...   \n",
       "150                                    0.415566   \n",
       "151                                    0.149361   \n",
       "152                                    0.065258   \n",
       "153                                    0.170653   \n",
       "154                                    0.414957   \n",
       "\n",
       "     T1fs_original_gldm_LargeDependenceLowGrayLevelEmphasis  \\\n",
       "ID                                                            \n",
       "0                                             0.029379        \n",
       "1                                            -0.002527        \n",
       "2                                             0.030527        \n",
       "3                                             0.001853        \n",
       "4                                             0.078517        \n",
       "..                                                 ...        \n",
       "150                                           0.151211        \n",
       "151                                           0.053821        \n",
       "152                                           0.010759        \n",
       "153                                           0.068206        \n",
       "154                                           0.155286        \n",
       "\n",
       "     T1fs_original_gldm_LowGrayLevelEmphasis  \\\n",
       "ID                                             \n",
       "0                                   0.069781   \n",
       "1                                   0.025307   \n",
       "2                                   0.107069   \n",
       "3                                   0.016859   \n",
       "4                                   0.110207   \n",
       "..                                       ...   \n",
       "150                                 0.197115   \n",
       "151                                 0.157847   \n",
       "152                                 0.031444   \n",
       "153                                 0.178974   \n",
       "154                                 0.182980   \n",
       "\n",
       "     T1fs_original_gldm_SmallDependenceEmphasis  \\\n",
       "ID                                                \n",
       "0                                      0.051594   \n",
       "1                                      0.171700   \n",
       "2                                      0.156891   \n",
       "3                                      0.120955   \n",
       "4                                      0.039649   \n",
       "..                                          ...   \n",
       "150                                    0.057972   \n",
       "151                                    0.136706   \n",
       "152                                    0.206859   \n",
       "153                                    0.109040   \n",
       "154                                    0.038477   \n",
       "\n",
       "     T1fs_original_gldm_SmallDependenceHighGrayLevelEmphasis  \\\n",
       "ID                                                             \n",
       "0                                             0.001241         \n",
       "1                                             0.050888         \n",
       "2                                             0.031479         \n",
       "3                                             0.069555         \n",
       "4                                             0.000776         \n",
       "..                                                 ...         \n",
       "150                                           0.004302         \n",
       "151                                           0.021127         \n",
       "152                                           0.096794         \n",
       "153                                           0.024246         \n",
       "154                                           0.010731         \n",
       "\n",
       "     T1fs_original_gldm_SmallDependenceLowGrayLevelEmphasis  \n",
       "ID                                                           \n",
       "0                                             0.019483       \n",
       "1                                             0.030947       \n",
       "2                                             0.062275       \n",
       "3                                             0.003478       \n",
       "4                                             0.014309       \n",
       "..                                                 ...       \n",
       "150                                           0.066838       \n",
       "151                                           0.127816       \n",
       "152                                           0.041305       \n",
       "153                                           0.098005       \n",
       "154                                           0.034973       \n",
       "\n",
       "[155 rows x 290 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e85d38de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80:20 negative to positive ratio\n"
     ]
    }
   ],
   "source": [
    "# class imbalance\n",
    "alts = len(df[df['Type'] == 1])\n",
    "lipomas = len(df) - alts\n",
    "total = len(df)\n",
    "print(f'{str(\"{:.0f}\".format(lipomas/total*100))}:{str(\"{:.0f}\".format(alts/total*100))} negative to positive ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "271d0bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(df.columns[2:])\n",
    "\n",
    "y = df['Type']\n",
    "y = pd.DataFrame.to_numpy(y) # data object: numpy array\n",
    "\n",
    "X = df.drop(df.columns[[0, 1]], axis=1)\n",
    "X = pd.DataFrame.to_numpy(X) # data object: numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db30d567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotCM(cm, i, loop, ct_o, ct_i, k, l):\n",
    "    display = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    display.plot()\n",
    "    plt.title(\"Confusion matrix \" + r\"$\\bf{\" + loop + \"}$\" + \" loop\")\n",
    "    plt.savefig(f'../../results/svm/combined/svm-ncv{i}-cm-{loop}-o{ct_o}i{ct_i}-Kernel-{k}-C-{l}.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79f0cc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrmr(xTrain, xTest, yTrain, given_features, K):\n",
    "    #    X: pandas.DataFrame, features\n",
    "    #    y: pandas.Series, target variable\n",
    "    #    K: number of features to select\n",
    "    \n",
    "    xTrain = pd.DataFrame(data=xTrain, columns=given_features)\n",
    "    \n",
    "    # only to filter it at the end\n",
    "    xTest = pd.DataFrame(data=xTest, columns=given_features)\n",
    "    \n",
    "    # for F-statistic\n",
    "    y = pd.Series(yTrain)\n",
    "\n",
    "    # compute F-statistics and initialize correlation matrix\n",
    "    F = pd.Series(f_regression(xTrain, y)[0], index = xTrain.columns)\n",
    "    corr = pd.DataFrame(.00001, index = xTrain.columns, columns = xTrain.columns)\n",
    "    \n",
    "    # initialize list of selected features and list of excluded features\n",
    "    selected = []\n",
    "    not_selected = xTrain.columns.to_list()\n",
    "    \n",
    "    # repeat K times\n",
    "    for i in range(K):\n",
    "        # compute (absolute) correlations between the last selected feature and all the (currently) excluded features\n",
    "        if i > 0:\n",
    "            last_selected = selected[-1]\n",
    "            corr.loc[not_selected, last_selected] = xTrain[not_selected].corrwith(xTrain[last_selected]).abs().clip(.00001)\n",
    "            \n",
    "        # compute FCQ score for all the (currently) excluded features (this is Formula 2)\n",
    "        score = F.loc[not_selected] / corr.loc[not_selected, selected].mean(axis = 1).fillna(.00001)\n",
    "        \n",
    "        # find best feature, add it to selected and remove it from not_selected\n",
    "        best = score.index[score.argmax()]\n",
    "        selected.append(best)\n",
    "        not_selected.remove(best)\n",
    "        \n",
    "    # filter columns\n",
    "    xTrain_filtered = xTrain.drop(not_selected, axis = 1)\n",
    "    xTest_filtered = xTest.drop(not_selected, axis = 1)\n",
    "    \n",
    "    # print(\"Features: \", *xTrain_filtered.columns, sep='\\n')\n",
    "    \n",
    "    # convert back to numpy array\n",
    "    xTrain_filtered = pd.DataFrame.to_numpy(xTrain_filtered)\n",
    "    xTest_filtered = pd.DataFrame.to_numpy(xTest_filtered)\n",
    "    \n",
    "    # print(\"!!!!\", xTrain_filtered.shape, xTest_filtered.shape)\n",
    "        \n",
    "    return xTrain_filtered, xTest_filtered, selected, not_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11ea1729",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../../results/svm/combined/svm-output.txt\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7035c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_vis(X_train_i, features):\n",
    "        X_train_i = pd.DataFrame(data=X_train_i, columns=features)\n",
    "        tot_var = 0.95 # total variance\n",
    "        pca_model = PCA(n_components = tot_var)\n",
    "        \n",
    "        X_train_pca = PCA(tot_var, svd_solver = 'full').fit(X_train_i)\n",
    "        \n",
    "        # print(\"Variance ratio:\", X_train_pca.explained_variance_ratio_); print()\n",
    "        # print(\"PCA dimensions:\", X_train_pca.components_.shape[0]); print()\n",
    "        # print(\"Reduced dimensions can explain {:.4f}\".format(sum(X_train_pca.explained_variance_ratio_)*100),\n",
    "        #       \"% of the variance in the original data.\"); print()\n",
    "        \n",
    "        # components\n",
    "        n_pcs= X_train_pca.components_.shape[0]\n",
    "        \n",
    "        # PCA coverts the features in array format; so, if we want to get the feature names:\n",
    "        # most_important = [np.abs(X_train_pca.components_[i]).argmax() for i in range(n_pcs)]\n",
    "        # most_important_names = [features[most_important[i]] for i in range(n_pcs)]\n",
    "        # dic = {'PC{}'.format(i): most_important_names[i] for i in range(n_pcs)}\n",
    "        # p = pd.DataFrame(dic.items())\n",
    "        # # print(\"New dimensions:\\n\", p); print()\n",
    "        # \n",
    "        # X_train_i = pd.DataFrame.to_numpy(X_train_i) # data object: numpy array\n",
    "        \n",
    "        return n_pcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ab599ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "def plot_learning_curve(\n",
    "    train_sizes_ncv,\n",
    "    train_scores_mean,\n",
    "    train_scores_std,\n",
    "    test_scores_mean,\n",
    "    test_scores_std,\n",
    "    fit_times_mean,\n",
    "    fit_times_std,\n",
    "    axes=None,\n",
    "    ylim=None,\n",
    "    cv=None,\n",
    "    n_jobs=None,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate 3 plots: the test and training learning curve, the training\n",
    "    samples vs fit times curve, the fit times vs score curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator instance\n",
    "        An estimator instance implementing `fit` and `predict` methods which\n",
    "        will be cloned for each validation.\n",
    "\n",
    "    title : str\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like of shape (n_samples, n_features)\n",
    "        Training vector, where ``n_samples`` is the number of samples and\n",
    "        ``n_features`` is the number of features.\n",
    "\n",
    "    y : array-like of shape (n_samples) or (n_samples, n_features)\n",
    "        Target relative to ``X`` for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    axes : array-like of shape (3,), default=None\n",
    "        Axes to use for plotting the curves.\n",
    "\n",
    "    ylim : tuple of shape (2,), default=None\n",
    "        Defines minimum and maximum y-values plotted, e.g. (ymin, ymax).\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, default=None\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "\n",
    "          - None, to use the default 5-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - :term:`CV splitter`,\n",
    "          - An iterable yielding (train, test) splits as arrays of indices.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : int or None, default=None\n",
    "        Number of jobs to run in parallel.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    train_sizes : array-like of shape (n_ticks,)\n",
    "        Relative or absolute numbers of training examples that will be used to\n",
    "        generate the learning curve. If the ``dtype`` is float, it is regarded\n",
    "        as a fraction of the maximum size of the training set (that is\n",
    "        determined by the selected validation method), i.e. it has to be within\n",
    "        (0, 1]. Otherwise it is interpreted as absolute sizes of the training\n",
    "        sets. Note that for classification the number of samples usually have\n",
    "        to be big enough to contain at least one sample from each class.\n",
    "        (default: np.linspace(0.1, 1.0, 5))\n",
    "    \"\"\"\n",
    "    if axes is None:\n",
    "        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "    axes[0].set_title(\"SVC Learning Curve\")\n",
    "    if ylim is not None:\n",
    "        axes[0].set_ylim(*ylim)\n",
    "    axes[0].set_xlabel(\"Training examples\")\n",
    "    axes[0].set_ylabel(\"Score\")\n",
    "\n",
    "    \n",
    "    # train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(\n",
    "    #     estimator,\n",
    "    #     X,\n",
    "    #     y,\n",
    "    #     cv=cv,\n",
    "    #     n_jobs=n_jobs,\n",
    "    #     train_sizes=train_sizes,\n",
    "    #     return_times=True,\n",
    "    # )\n",
    "    # train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    # train_scores_std = np.std(train_scores, axis=1)\n",
    "    # test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    # test_scores_std = np.std(test_scores, axis=1)\n",
    "    # fit_times_mean = np.mean(fit_times, axis=1)\n",
    "    # fit_times_std = np.std(fit_times, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes[0].grid()\n",
    "    axes[0].fill_between(\n",
    "        train_sizes_ncv,\n",
    "        train_scores_mean - train_scores_std,\n",
    "        train_scores_mean + train_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"b\",\n",
    "    )\n",
    "    axes[0].fill_between(\n",
    "        train_sizes_ncv,\n",
    "        test_scores_mean - test_scores_std,\n",
    "        test_scores_mean + test_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"g\",\n",
    "    )\n",
    "    axes[0].plot(\n",
    "        train_sizes_ncv, train_scores_mean, \"o-\", color=\"b\", label=\"Training score\"\n",
    "    )\n",
    "    axes[0].plot(\n",
    "        train_sizes_ncv, test_scores_mean, \"o-\", color=\"g\", label=\"Cross-validation score\"\n",
    "    )\n",
    "    axes[0].legend(loc=\"best\")\n",
    "\n",
    "    # Plot n_samples vs fit_times\n",
    "    axes[1].grid()\n",
    "    axes[1].plot(train_sizes_ncv, fit_times_mean, \"o-\")\n",
    "    axes[1].fill_between(\n",
    "        train_sizes_ncv,\n",
    "        fit_times_mean - fit_times_std,\n",
    "        fit_times_mean + fit_times_std,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    axes[1].set_xlabel(\"Training examples\")\n",
    "    axes[1].set_ylabel(\"fit_times\")\n",
    "    axes[1].set_title(\"Scalability of the model\")\n",
    "\n",
    "    # Plot fit_time vs score\n",
    "    fit_time_argsort = fit_times_mean.argsort()\n",
    "    fit_time_sorted = fit_times_mean[fit_time_argsort]\n",
    "    test_scores_mean_sorted = test_scores_mean[fit_time_argsort]\n",
    "    test_scores_std_sorted = test_scores_std[fit_time_argsort]\n",
    "    axes[2].grid()\n",
    "    axes[2].plot(fit_time_sorted, test_scores_mean_sorted, \"o-\")\n",
    "    axes[2].fill_between(\n",
    "        fit_time_sorted,\n",
    "        test_scores_mean_sorted - test_scores_std_sorted,\n",
    "        test_scores_mean_sorted + test_scores_std_sorted,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    axes[2].set_xlabel(\"fit_times\")\n",
    "    axes[2].set_ylabel(\"Score\")\n",
    "    axes[2].set_title(\"Performance of the model\")\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3ffefe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the nested CV procedure\n",
    "# dataset D=(X, y)\n",
    "\n",
    "# define number of folds based on dataset size and keep folds while looping through different models\n",
    "K1 = 3 # outer\n",
    "K2 = 3 # inner\n",
    "\n",
    "# define the model\n",
    "model = SVC(probability=True)\n",
    "\n",
    "# define a search space with at least 2 parameters so we have a 2D grid space\n",
    "space = dict()\n",
    "space['kernel'] = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "space['C'] = [1000, 100, 10, 1.0, 0.1, 0.01, 0.001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c11ba455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of all possible hyperparameter combinations\n",
    "keys, values = zip(*space.items())\n",
    "search_space = [dict(zip(keys, v)) for v in itertools.product(*values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6d4c916",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# GridSearchCV custom implementation\n",
    "series = range(1, 51) # for multiple runs of CV\n",
    "\n",
    "# metrics across all 150 models of the 50 nCV runs\n",
    "accuracy_ncv  = []\n",
    "balanced_accuracy_ncv  = []  \n",
    "f1_ncv  = []\n",
    "recall_ncv  = []\n",
    "mcc_ncv = []\n",
    "sensitivity_ncv = []\n",
    "specificity_ncv = []\n",
    "\n",
    "hyperparameters = []\n",
    "hp_features = []\n",
    "\n",
    "mean_accuracy = {}\n",
    "mean_balanced_accuracy = {}\n",
    "mean_f1 = {}\n",
    "mean_recall = {}\n",
    "mean_mcc = {}\n",
    "\n",
    "# ROC -------------------------------\n",
    "tprs = []\n",
    "aucs = []\n",
    "base_fpr = np.linspace(0, 1, 101)\n",
    "colors = ['royalblue']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# -----------------------------------\n",
    "\n",
    "# calibration curve -----------------\n",
    "probs_true = []\n",
    "probs_pred = []\n",
    "# -----------------------------------\n",
    "\n",
    "# learning curve --------------------\n",
    "train_sizes_ncv = []\n",
    "train_scores_ncv = []\n",
    "test_scores_ncv = []\n",
    "fit_times_ncv = []\n",
    "# -----------------------------------\n",
    "\n",
    "# models to file\n",
    "g = open('/Users/dianadavid/Uni/Thesis/repo/results/svm/combined/svm_final_models_combined.dat', 'wb')\n",
    "\n",
    "start_time = time.time()\n",
    "for i in series:\n",
    "    cv_outer = StratifiedKFold(n_splits=K1, random_state=i, shuffle=True)\n",
    "    cv_inner = StratifiedKFold(n_splits=K2, random_state=i, shuffle=True)\n",
    "\n",
    "    f.write(f'\\n\\033[1m NESTED CV RUN #{i} \\033[0m')\n",
    "    \n",
    "    # outer loop metrics\n",
    "    accuracy_outer_folds  = []\n",
    "    balanced_accuracy_outer_folds  = []  \n",
    "    f1_outer_folds  = []\n",
    "    recall_outer_folds  = []\n",
    "    mcc_outer_folds = []\n",
    "    \n",
    "    outer_hp = {}\n",
    "\n",
    "    # OUTER LOOP\n",
    "    ct_o = 0\n",
    "    for train_indices, test_indices in cv_outer.split(X, y):\n",
    "        ct_o = ct_o + 1\n",
    "        f.write(f'\\n----------- Outer loop #{ct_o} -----------')\n",
    "        \n",
    "        kernels = []\n",
    "        c_params = []\n",
    "        internal_hp = {}\n",
    "        \n",
    "        # inner loop metrics\n",
    "        best_score = 0.0\n",
    "        best_hp = search_space[0]\n",
    "        best_feature_set = []\n",
    "        balanced_accuracy_inner_folds  = []\n",
    "        \n",
    "        selected_features = []\n",
    "        not_selected_features = []\n",
    "        \n",
    "        # print(\"Train indices: \", train_indices, \"\\nTest indices: \", test_indices, \"\\n\\n\")\n",
    "        X_train_i, X_test_i = X[train_indices], X[test_indices]\n",
    "        y_train_i, y_test_i = y[train_indices], y[test_indices]\n",
    " \n",
    "        ct_i = 0\n",
    "        # INNER LOOP\n",
    "        for train_indices_inner, test_indices_inner in cv_inner.split(X_train_i, y_train_i):\n",
    "            ct_i = ct_i + 1\n",
    "            f.write(f'\\nInner loop #{ct_i}')\n",
    "            \n",
    "            # print(\"Train indices inner: \", train_indices_inner, \"\\nTest indices inner: \", test_indices_inner, \"\\n\\n\")\n",
    "            X_train_j, X_test_j = X_train_i[train_indices_inner], X_train_i[test_indices_inner]\n",
    "            y_train_j, y_test_j = y_train_i[train_indices_inner], y_train_i[test_indices_inner]\n",
    "                                    \n",
    "            # PCA visualisation ------------------------------------------------------------------------------------\n",
    "            dimensions = pca_vis(X_train_j, features)\n",
    "\n",
    "            # MRMR\n",
    "            X_train_j, X_test_j, selected_inner_features, not_selected_inner_features = mrmr(X_train_j, X_test_j, y_train_j, features, dimensions)\n",
    "            # print(\"MRMR: \", dimensions)  \n",
    "            \n",
    "            # oversampling -------------------------------------------------------------------------------------------\n",
    "            # X_train_j, y_train_j = oversampling(X_train_j, y_train_j, ct_o, ct_i)\n",
    "            \n",
    "            # over- and undersampling\n",
    "            over = SMOTE(sampling_strategy=0.6)\n",
    "            under = RandomUnderSampler(sampling_strategy=0.8)\n",
    "            steps = [('over', over), ('under', under)]\n",
    "            pipeline = Pipeline(steps=steps)\n",
    "            X_train_j, y_train_j = pipeline.fit_resample(X_train_j, y_train_j)\n",
    "            \n",
    "            f.write(f'\\n# of selected inner features: {len(selected_inner_features)}')\n",
    "            selected_features.append(selected_inner_features) # list of lists to compare\n",
    "            \n",
    "            not_selected_features.append(not_selected_inner_features)\n",
    "   \n",
    "            # --------------------------------------------------------------------------------------------------------\n",
    "            \n",
    "            for item in search_space:\n",
    "                model.set_params(**item)\n",
    "                model.fit(X_train_j, y_train_j)\n",
    "\n",
    "                y_predicted_inner_test = model.predict(X_test_j)\n",
    "                \n",
    "                score = balanced_accuracy_score(y_test_j, y_predicted_inner_test)\n",
    "                \n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_hp = item\n",
    "                    best_feature_set = selected_inner_features\n",
    "                    \n",
    "            \n",
    "            f.write(\"\\n------------------------------------ end of inner loop ------------------------------------\")\n",
    "          \n",
    "        # feature selection ------------------------------------------------------------------------------------        \n",
    "        X_train_i = pd.DataFrame(data=X_train_i, columns=features)\n",
    "        X_test_i = pd.DataFrame(data=X_test_i, columns=features)\n",
    "        \n",
    "        cols = [col for col in X_train_i.columns if col in best_feature_set]\n",
    "        \n",
    "        # print(f'\\n#common features: {len(set(best_features) & set(cols))}')\n",
    "        \n",
    "        X_train_i = X_train_i[cols]\n",
    "        X_test_i = X_test_i[cols]\n",
    "        \n",
    "        X_train_i = pd.DataFrame.to_numpy(X_train_i)\n",
    "        X_test_i = pd.DataFrame.to_numpy(X_test_i)\n",
    "        # ------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        # set params(best item from inner)\n",
    "        model.set_params(**best_hp)\n",
    "        \n",
    "        hyperparameters.append(best_hp)\n",
    "        hp_features.append(best_feature_set)\n",
    "        \n",
    "        model.fit(X_train_i, y_train_i)\n",
    "        \n",
    "        pickle.dump(model, g)\n",
    "        # print(\"\\n\", model)\n",
    "\n",
    "        y_predicted_outer_test = model.predict(X_test_i)\n",
    "        \n",
    "        # metrics\n",
    "        accuracy_outer_folds.append(accuracy_score(y_test_i, y_predicted_outer_test))\n",
    "        balanced_accuracy_outer_folds.append(balanced_accuracy_score(y_test_i, y_predicted_outer_test))\n",
    "        f1_outer_folds.append(f1_score(y_test_i, y_predicted_outer_test))\n",
    "        recall_outer_folds.append(recall_score(y_test_i, y_predicted_outer_test))\n",
    "        mcc_outer_folds.append(matthews_corrcoef(y_test_i, y_predicted_outer_test))\n",
    "\n",
    "        cm_o = confusion_matrix(y_test_i, y_predicted_outer_test)\n",
    "        # plotCM(cm_o, i, \"outer\", ct_o, 'x', 0, 0)\n",
    "        \n",
    "        sensitivity_o = cm_o[1,1]/(cm_o[1,1]+cm_o[1,0]) # TP/(TP+TN)\n",
    "        f.write(f'\\nSensitivity: {round(sensitivity_o*100, 2)}%')\n",
    "\n",
    "        specificity_o = cm_o[0,0]/(cm_o[0,0]+cm_o[0,1]) # TN/(TN+FP)\n",
    "        f.write(f'\\nSpecificity: {round(specificity_o*100, 2)}%')\n",
    "        \n",
    "        sensitivity_ncv.append(sensitivity_o)\n",
    "        specificity_ncv.append(specificity_o)\n",
    "        \n",
    "        # ROC ------------------------------------------------------------------------------------------------\n",
    "        y_score = model.predict_proba(X_test_i)\n",
    "        fpr, tpr, _ = roc_curve(y_test_i, y_score[:, 1])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        aucs.append(roc_auc)\n",
    "        #plt.plot(fpr, tpr, lw=1, alpha=0.6, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc), c = colors[i])\n",
    "        tpr = interp(base_fpr, fpr, tpr)\n",
    "        tpr[0] = 0.0\n",
    "        tprs.append(tpr)\n",
    "        # ----------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        # calibration curve ----------------------------------------------------------------------------------\n",
    "        prob_true, prob_pred = calibration_curve(y_test_i, y_score[:,1], strategy='quantile')\n",
    "        probs_true.append(prob_true)\n",
    "        probs_pred.append(prob_pred)\n",
    "        # ----------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        # learning curve -------------------------------------------------------------------------------------\n",
    "        train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(\n",
    "            model,\n",
    "            X_train_i,\n",
    "            y_train_i,\n",
    "            cv=cv_outer,\n",
    "            n_jobs=4,\n",
    "            train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "            return_times=True,\n",
    "        )\n",
    "        train_sizes_ncv.append(train_sizes)\n",
    "        train_scores_ncv.append(train_scores)\n",
    "        test_scores_ncv.append(test_scores)\n",
    "        fit_times_ncv.append(fit_times)\n",
    "        # ------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        # check how many features were selected in all loops\n",
    "        f.write(f'\\n# common features: {len(set(selected_features[0]) & set(selected_features[1]) & set(selected_features[2]))}')\n",
    "        \n",
    "        f.write(\"\\n------------------------------------ end of outer loop ------------------------------------\")\n",
    "        \n",
    "    f.write(f'\\nBest balanced accuracy: {best_score}')\n",
    "    f.write(f'\\nBest kernel: {model.kernel}')\n",
    "    f.write(f'\\nBest C: {model.C}')\n",
    "    \n",
    "    accuracy_ncv.append(accuracy_outer_folds)\n",
    "    balanced_accuracy_ncv.append(balanced_accuracy_outer_folds)\n",
    "    f1_ncv.append(f1_outer_folds)\n",
    "    recall_ncv.append(recall_outer_folds)\n",
    "    mcc_ncv.append(mcc_outer_folds)\n",
    "    \n",
    "    mean_accuracy['Nested CV run #' + str(i)] = [str(\"{:.2f}\".format(np.mean(np.array(accuracy_outer_folds)))) +  \" +/- \" + str(\"{:.2f}\".format(np.std(np.array(accuracy_outer_folds))))]\n",
    "    mean_balanced_accuracy['Nested CV run #' + str(i)] = [str(\"{:.2f}\".format(np.mean(np.array(balanced_accuracy_outer_folds)))) +  \" +/- \" + str(\"{:.2f}\".format(np.std(np.array(balanced_accuracy_outer_folds))))]\n",
    "    mean_f1['Nested CV run #' + str(i)] = [str(\"{:.2f}\".format(np.mean(np.array(f1_outer_folds)))) +  \" +/- \" + str(\"{:.2f}\".format(np.std(np.array(f1_outer_folds))))]\n",
    "    mean_recall['Nested CV run #' + str(i)] = [str(\"{:.2f}\".format(np.mean(np.array(recall_outer_folds)))) +  \" +/- \" + str(\"{:.2f}\".format(np.std(np.array(recall_outer_folds))))]\n",
    "    mean_mcc['Nested CV run #' + str(i)] = [str(\"{:.2f}\".format(np.mean(np.array(mcc_outer_folds)))) +  \" +/- \" + str(\"{:.2f}\".format(np.std(np.array(mcc_outer_folds))))]\n",
    "    \n",
    "    f.write(\"\\n---------------------------------- end of nested cv run -----------------------------------\")\n",
    "    \n",
    "g.close()\n",
    "\n",
    "ff = open('../../results/svm/combined/svm_features_combined.txt', 'w')\n",
    "for item in hp_features:\n",
    "    ff.write(str(item) + \"\\n\")\n",
    "ff.close()\n",
    "\n",
    "gg = open('../../results/svm/combined/svm_hyperparams_combined.txt', 'w')\n",
    "for item in hyperparameters:\n",
    "    gg.write(str(item) + \"\\n\")\n",
    "gg.close()\n",
    "         \n",
    "# learning curve ---------------------------------------------------------------------------------------------\n",
    "train_sizes_ncv = np.mean(train_sizes_ncv, axis=0)\n",
    "train_sizes_ncv = np.around(train_sizes_ncv)\n",
    "train_sizes_ncv = train_sizes_ncv.astype(int)\n",
    "\n",
    "train_scores_mean = []\n",
    "train_scores_std = []\n",
    "tr_sc1 = []\n",
    "tr_sc2 = []\n",
    "tr_sc3 = []\n",
    "tr_sc4 = []\n",
    "tr_sc5 = []\n",
    "\n",
    "test_scores_mean = []\n",
    "test_scores_std = []\n",
    "t_sc1 = []\n",
    "t_sc2 = []\n",
    "t_sc3 = []\n",
    "t_sc4 = []\n",
    "t_sc5 = []\n",
    "\n",
    "fit_times_mean = []\n",
    "fit_times_std = []\n",
    "f_sc1 = []\n",
    "f_sc2 = []\n",
    "f_sc3 = []\n",
    "f_sc4 = []\n",
    "f_sc5 = []\n",
    "\n",
    "for i in range(len(train_scores_ncv)-1):\n",
    "    tr_sc1.append(train_scores_ncv[i][0])\n",
    "    tr_sc2.append(train_scores_ncv[i][1])\n",
    "    tr_sc3.append(train_scores_ncv[i][2])\n",
    "    tr_sc4.append(train_scores_ncv[i][3])\n",
    "    tr_sc5.append(train_scores_ncv[i][4])\n",
    "    \n",
    "    t_sc1.append(test_scores_ncv[i][0])\n",
    "    t_sc2.append(test_scores_ncv[i][1])\n",
    "    t_sc3.append(test_scores_ncv[i][2])\n",
    "    t_sc4.append(test_scores_ncv[i][3])\n",
    "    t_sc5.append(test_scores_ncv[i][4])\n",
    "    \n",
    "    f_sc1.append(fit_times_ncv[i][0])\n",
    "    f_sc2.append(fit_times_ncv[i][1])\n",
    "    f_sc3.append(fit_times_ncv[i][2])\n",
    "    f_sc4.append(fit_times_ncv[i][3])\n",
    "    f_sc5.append(fit_times_ncv[i][4])  \n",
    "    \n",
    "tr_sc1 = np.ravel(tr_sc1)\n",
    "tr_sc1_std = np.std(tr_sc1)\n",
    "tr_sc1 = np.mean(tr_sc1) \n",
    "\n",
    "train_scores_mean.append(tr_sc1)\n",
    "train_scores_std.append(tr_sc1_std)\n",
    "\n",
    "tr_sc2 = np.ravel(tr_sc2)\n",
    "tr_sc2_std = np.std(tr_sc2)\n",
    "tr_sc2 = np.mean(tr_sc2) \n",
    "\n",
    "train_scores_mean.append(tr_sc2)\n",
    "train_scores_std.append(tr_sc2_std)\n",
    "\n",
    "tr_sc3 = np.ravel(tr_sc3)\n",
    "tr_sc3_std = np.std(tr_sc3)\n",
    "tr_sc3 = np.mean(tr_sc3) \n",
    "\n",
    "train_scores_mean.append(tr_sc3)\n",
    "train_scores_std.append(tr_sc3_std)\n",
    "\n",
    "tr_sc4 = np.ravel(tr_sc4)\n",
    "tr_sc4_std = np.std(tr_sc4)\n",
    "tr_sc4 = np.mean(tr_sc4) \n",
    "\n",
    "train_scores_mean.append(tr_sc4)\n",
    "train_scores_std.append(tr_sc4_std)\n",
    "\n",
    "tr_sc5 = np.ravel(tr_sc5)\n",
    "tr_sc5_std = np.std(tr_sc5)\n",
    "tr_sc5 = np.mean(tr_sc5) \n",
    "\n",
    "train_scores_mean.append(tr_sc5)\n",
    "train_scores_mean = np.array(train_scores_mean)\n",
    "train_scores_std.append(tr_sc5_std)\n",
    "train_scores_std = np.array(train_scores_std)\n",
    "    \n",
    "# print(\"Train scores mean: \", train_scores_mean) \n",
    "# print(\"Train scores std: \", train_scores_std) \n",
    "\n",
    "t_sc1 = np.ravel(t_sc1)\n",
    "t_sc1_std = np.std(t_sc1)\n",
    "t_sc1 = np.mean(t_sc1) \n",
    "\n",
    "test_scores_mean.append(t_sc1)\n",
    "test_scores_std.append(t_sc1_std)\n",
    "\n",
    "t_sc2 = np.ravel(t_sc2)\n",
    "t_sc2_std = np.std(t_sc2)\n",
    "t_sc2 = np.mean(t_sc2) \n",
    "\n",
    "test_scores_mean.append(t_sc2)\n",
    "test_scores_std.append(t_sc2_std)\n",
    "\n",
    "t_sc3 = np.ravel(t_sc3)\n",
    "t_sc3_std = np.std(t_sc3)\n",
    "t_sc3 = np.mean(t_sc3) \n",
    "\n",
    "test_scores_mean.append(t_sc3)\n",
    "test_scores_std.append(t_sc3_std)\n",
    "\n",
    "t_sc4 = np.ravel(t_sc4)\n",
    "t_sc4_std = np.std(t_sc4)\n",
    "t_sc4 = np.mean(t_sc4) \n",
    "\n",
    "test_scores_mean.append(t_sc4)\n",
    "test_scores_std.append(t_sc4_std)\n",
    "\n",
    "t_sc5 = np.ravel(t_sc5)\n",
    "t_sc5_std = np.std(t_sc5)\n",
    "t_sc5 = np.mean(t_sc5) \n",
    "\n",
    "test_scores_mean.append(t_sc5)\n",
    "test_scores_mean = np.array(test_scores_mean)\n",
    "test_scores_std.append(t_sc5_std)\n",
    "test_scores_std = np.array(test_scores_std)\n",
    "    \n",
    "# print(\"Test scores mean: \", test_scores_mean) \n",
    "# print(\"Test scores std: \", test_scores_std) \n",
    "\n",
    "f_sc1 = np.ravel(f_sc1)\n",
    "f_sc1_std = np.std(f_sc1)\n",
    "f_sc1 = np.mean(f_sc1) \n",
    "\n",
    "fit_times_mean.append(f_sc1)\n",
    "fit_times_std.append(f_sc1_std)\n",
    "\n",
    "f_sc2 = np.ravel(f_sc2)\n",
    "f_sc2_std = np.std(f_sc2)\n",
    "f_sc2 = np.mean(f_sc2) \n",
    "\n",
    "fit_times_mean.append(f_sc2)\n",
    "fit_times_std.append(f_sc2_std)\n",
    "\n",
    "f_sc3 = np.ravel(f_sc3)\n",
    "f_sc3_std = np.std(f_sc3)\n",
    "f_sc3 = np.mean(f_sc3) \n",
    "\n",
    "fit_times_mean.append(f_sc3)\n",
    "fit_times_std.append(f_sc3_std)\n",
    "\n",
    "f_sc4 = np.ravel(f_sc4)\n",
    "f_sc4_std = np.std(f_sc4)\n",
    "f_sc4 = np.mean(f_sc4) \n",
    "\n",
    "fit_times_mean.append(f_sc4)\n",
    "fit_times_std.append(f_sc4_std)\n",
    "\n",
    "f_sc5 = np.ravel(f_sc5)\n",
    "f_sc5_std = np.std(f_sc5)\n",
    "f_sc5 = np.mean(f_sc5) \n",
    "\n",
    "fit_times_mean.append(f_sc5)\n",
    "fit_times_mean = np.array(fit_times_mean)\n",
    "fit_times_std.append(f_sc5_std)\n",
    "fit_times_std = np.array(fit_times_std)\n",
    "    \n",
    "# print(\"Fit times mean: \", fit_times_mean) \n",
    "# print(\"Fit times std: \", fit_times_std) \n",
    "    \n",
    "plot_learning_curve(train_sizes_ncv, train_scores_mean, train_scores_std, test_scores_mean, test_scores_std, \n",
    "                    fit_times_mean, fit_times_std, cv=cv_outer, n_jobs=4)\n",
    "plt.savefig(f'../../results/svm/combined/ncv-svm-learning-curve.png')\n",
    "plt.close()\n",
    "# -----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# calibration curve -----------------------------------------------------------------------------------------\n",
    "fig, ax = plt.subplots()\n",
    "probs_pred = np.array(probs_pred)\n",
    "probs_true = np.array(probs_true)\n",
    "probs_pred = probs_pred.mean(axis=0)\n",
    "probs_true = probs_true.mean(axis=0)\n",
    "plt.plot(probs_pred, probs_true, marker='o', linewidth=1, label='SVC')\n",
    "\n",
    "line = mlines.Line2D([0, 1], [0, 1], color='black', label='Perfectly Calibrated', linestyle = '--')\n",
    "transform = ax.transAxes\n",
    "line.set_transform(transform)\n",
    "ax.add_line(line)\n",
    "fig.suptitle('Calibration plot')\n",
    "ax.set_xlabel('Predicted probability')\n",
    "ax.set_ylabel('True probability in each bin')\n",
    "plt.legend()\n",
    "# plt.show()\n",
    "plt.savefig(f'../../results/svm/combined/svm-cal-curve.png')\n",
    "plt.close()\n",
    "# -----------------------------------------------------------------------------------------------------------\n",
    " \n",
    "# ROC -------------------------------------------------------------------------------------------------------\n",
    "tprs = np.array(tprs)\n",
    "mean_tprs = tprs.mean(axis=0)\n",
    "std = tprs.std(axis=0)\n",
    "\n",
    "mean_auc = auc(base_fpr, mean_tprs)\n",
    "std_auc = np.std(aucs)\n",
    "\n",
    "tprs_upper = np.minimum(mean_tprs + std, 1)\n",
    "tprs_lower = mean_tprs - std\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(base_fpr, mean_tprs, 'royalblue', alpha = 0.8, label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),)\n",
    "plt.fill_between(base_fpr, tprs_lower, tprs_upper, color = 'grey', alpha = 0.2)\n",
    "plt.plot([0, 1], [0, 1], linestyle = '--', lw = 2, color = 'r', label = 'Chance', alpha= 0.8)\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title('Receiver operating characteristic (ROC) curve')\n",
    "#plt.axes().set_aspect('equal', 'datalim')\n",
    "# plt.show()\n",
    "plt.savefig(f'../../results/svm/combined/svm-roc.png')\n",
    "plt.close()\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "elapsed_time = (time.time() - start_time)\n",
    "f.write(f'\\nElapsed time: {elapsed_time} seconds \\n')\n",
    "\n",
    "f.write(f'Mean accuracy: \\n {mean_accuracy}')\n",
    "f.write(f'\\nMean balanced accuracy:  \\n {mean_balanced_accuracy}')\n",
    "f.write(f'\\nMean F1 score: \\n {mean_f1}')\n",
    "f.write(f'\\nMean recall: \\n {mean_recall}')\n",
    "f.write(f'\\nMean MCC: \\n {mean_mcc}')\n",
    "\n",
    "accuracy_ncv = np.ravel(accuracy_ncv)\n",
    "balanced_accuracy_ncv = np.ravel(balanced_accuracy_ncv)\n",
    "f1_ncv = np.ravel(f1_ncv)\n",
    "recall_ncv = np.ravel(recall_ncv)\n",
    "mcc_ncv = np.ravel(mcc_ncv)\n",
    "\n",
    "mean_accuracy_ncv = str(\"{:.2f}\".format(np.mean(np.array(accuracy_ncv)))) +  \" +/- \" + str(\"{:.2f}\".format(np.std(np.array(accuracy_ncv))))\n",
    "f.write(f'\\nMean accuracy across all folds: {mean_accuracy_ncv}') # all outer folds accs averaged - ???\n",
    "\n",
    "mean_balanced_accuracy_ncv = str(\"{:.2f}\".format(np.mean(np.array(balanced_accuracy_ncv)))) +  \" +/- \" + str(\"{:.2f}\".format(np.std(np.array(balanced_accuracy_ncv))))\n",
    "f.write(f'\\nMean balanced accuracy across all folds: {mean_balanced_accuracy_ncv}') # all outer folds bal accs averaged - ???\n",
    "\n",
    "mean_f1_ncv = str(\"{:.2f}\".format(np.mean(np.array(f1_ncv)))) +  \" +/- \" + str(\"{:.2f}\".format(np.std(np.array(f1_ncv))))\n",
    "f.write(f'\\nMean f1 across all folds: {mean_f1_ncv}') # all outer folds f1s averaged - ???\n",
    "\n",
    "mean_recall_ncv = str(\"{:.2f}\".format(np.mean(np.array(recall_ncv)))) +  \" +/- \" + str(\"{:.2f}\".format(np.std(np.array(recall_ncv))))\n",
    "f.write(f'\\nMean recall across all folds: {mean_recall_ncv}') # all outer folds recalls averaged - ???\n",
    "\n",
    "mean_mcc_ncv = str(\"{:.2f}\".format(np.mean(np.array(mcc_ncv)))) +  \" +/- \" + str(\"{:.2f}\".format(np.std(np.array(mcc_ncv))))\n",
    "f.write(f'\\nMean MCC across all folds: {mean_mcc_ncv}') # all outer folds mccs averaged - ???\n",
    "\n",
    "sensitivity_ncv = np.array(sensitivity_ncv)\n",
    "specificity_ncv = np.array(specificity_ncv)\n",
    "sensitivity_ncv = np.mean(sensitivity_ncv)\n",
    "specificity_ncv = np.mean(specificity_ncv)\n",
    "\n",
    "f.write(f'\\nMean sensitivity all folds: {round(sensitivity_ncv*100, 2)}%') \n",
    "f.write(f'\\nMean specificity all folds: {round(specificity_ncv*100, 2)}%') \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e518ef6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97bfed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = collections.defaultdict(int)  # 0 by default\n",
    "for x in itertools.chain.from_iterable(hp_features):\n",
    "    freq[x] += 1\n",
    "\n",
    "sorted_freqs = sorted(freq.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "freqs = []\n",
    "for i in sorted_freqs:\n",
    "    freqs.append([i[0], i[1]])\n",
    "    \n",
    "df_features = pd.DataFrame(freqs, columns = ['Feature', 'Selected'])\n",
    "df_features = df_features.iloc[0:10,:]\n",
    "\n",
    "import docx\n",
    "# open an existing document\n",
    "doc = docx.Document('./tables.docx')\n",
    "\n",
    "# add a table to the end and create a reference variable\n",
    "# extra row is so we can add the header row\n",
    "t = doc.add_table(df_features.shape[0]+1, df_features.shape[1])\n",
    "\n",
    "# add the header rows.\n",
    "for j in range(df_features.shape[-1]):\n",
    "    t.cell(0,j).text = df_features.columns[j]\n",
    "\n",
    "# add the rest of the data frame\n",
    "for i in range(df_features.shape[0]):\n",
    "    for j in range(df_features.shape[-1]):\n",
    "        t.cell(i+1,j).text = str(df_features.values[i,j])\n",
    "\n",
    "# save the doc\n",
    "doc.save('./tables.docx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df852653",
   "metadata": {},
   "source": [
    "# External Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a04ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ext = pd.read_csv(\"../../data/harmonized/combined_external_harmonized.csv\", sep= \"\\t\")\n",
    "df_ext2 = pd.read_csv(\"../../data/clinical/clinical_data_external_combined.csv\", sep=\";\", index_col=0)\n",
    "\n",
    "df_ext2['ID_intern'].replace(\"LIP\", \"\", regex=True, inplace=True)\n",
    "df_ext2.drop(df_ext2.columns[1:6], axis=1, inplace=True)\n",
    "df_ext2.drop(df_ext2.columns[2], axis=1, inplace=True)\n",
    "\n",
    "df_ext = df_ext2.join(df_ext.set_index('ID_intern'), on='ID_intern')\n",
    "\n",
    "df_ext = df_ext.rename({'Pathology': 'Type'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62488a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_ext = list(df_ext.columns[2:])\n",
    "\n",
    "y_ext = df_ext['Type']\n",
    "y_ext = pd.DataFrame.to_numpy(y_ext) # data object: numpy array\n",
    "\n",
    "X_ext = df_ext.drop(df_ext.columns[[0, 1]], axis=1)\n",
    "X_ext = pd.DataFrame.to_numpy(X_ext) # data object: numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eceb586",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "\n",
    "h = open('/Users/dianadavid/Uni/Thesis/repo/results/svm/combined/svm_final_models_combined.dat', 'rb')\n",
    "for item in range(150):\n",
    "    models.append(pickle.load(h))\n",
    "h.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fb19a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c47f7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(hp_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0973c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = open('accuracies_svm_models_combined.txt', 'w')\n",
    "c=0\n",
    "for model in models:\n",
    "    c = c + 1\n",
    "        \n",
    "    x = pd.DataFrame(data=X, columns=features)\n",
    "    x_ext = pd.DataFrame(data=X_ext, columns=features)\n",
    "    \n",
    "    # print(\"Params: \", item)\n",
    "    # print(\"Features: \", hp_features[ct - 1])\n",
    "    \n",
    "    cols = [col for col in x.columns if col in hp_features[c - 1]]\n",
    "    # print(f'\\n#common features for {ct-1}: {len(set(hp_features[ct - 1]) & set(cols))}')\n",
    "    \n",
    "    x = x[cols]\n",
    "    x_ext = x_ext[cols]\n",
    "\n",
    "    \n",
    "    x = pd.DataFrame.to_numpy(x)\n",
    "    x_ext = pd.DataFrame.to_numpy(x_ext)\n",
    "\n",
    "    \n",
    "    y_predicted = model.predict(x_ext)\n",
    "\n",
    "    k.write(\"\\n\" + str(balanced_accuracy_score(y_ext, y_predicted)))\n",
    "k.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6261dcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV to test on external dataset\n",
    "K=3\n",
    "\n",
    "cv = StratifiedKFold(n_splits=K, random_state=42, shuffle=True)\n",
    "\n",
    "hp = []\n",
    "feat = []\n",
    "\n",
    "# ROC -------------------------------\n",
    "tprs = []\n",
    "aucs = []\n",
    "base_fpr = np.linspace(0, 1, 101)\n",
    "colors = ['royalblue']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# -----------------------------------\n",
    "\n",
    "# calibration curve -----------------\n",
    "probs_true = []\n",
    "probs_pred = []\n",
    "# -----------------------------------\n",
    "\n",
    "\n",
    "for train_indices, test_indices in cv.split(X, y): \n",
    "    x_train, x_test = X[train_indices], X[test_indices]\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "    \n",
    "    accuracies = []\n",
    "    \n",
    "    ct = 0\n",
    "    for item in hyperparameters:\n",
    "        ct = ct + 1\n",
    "        \n",
    "        X_train = pd.DataFrame(data=x_train, columns=features)\n",
    "        X_test = pd.DataFrame(data=x_test, columns=features)\n",
    "        \n",
    "        # print(\"Params: \", item)\n",
    "        # print(\"Features: \", hp_features[ct - 1])\n",
    "        \n",
    "        cols = [col for col in X_train.columns if col in hp_features[ct - 1]]\n",
    "        # print(f'\\n#common features for {ct-1}: {len(set(hp_features[ct - 1]) & set(cols))}')\n",
    "        \n",
    "        X_train = X_train[cols]\n",
    "        X_test = X_test[cols]\n",
    "        \n",
    "        X_train = pd.DataFrame.to_numpy(X_train)\n",
    "        X_test = pd.DataFrame.to_numpy(X_test)\n",
    "    \n",
    "        model.set_params(**item)\n",
    "        model.fit(X_train, y_train) \n",
    "        \n",
    "        y_predicted = model.predict(X_test)\n",
    "        \n",
    "        \n",
    "        # ROC ------------------------------------------------------------------------------------------------\n",
    "        y_score = model.predict_proba(X_test)\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_score[:, 1])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        aucs.append(roc_auc)\n",
    "        #plt.plot(fpr, tpr, lw=1, alpha=0.6, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc), c = colors[i])\n",
    "        tpr = interp(base_fpr, fpr, tpr)\n",
    "        tpr[0] = 0.0\n",
    "        tprs.append(tpr)\n",
    "        # ----------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        # calibration curve ----------------------------------------------------------------------------------\n",
    "        prob_true, prob_pred = calibration_curve(y_test, y_score[:,1], strategy='quantile')\n",
    "        probs_true.append(prob_true)\n",
    "        probs_pred.append(prob_pred)\n",
    "        # ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "        accuracies.append(balanced_accuracy_score(y_test, y_predicted))\n",
    "         \n",
    "        hp.append(item)\n",
    "        feat.append(hp_features[ct - 1])\n",
    "             \n",
    "best_accuracy = np.amax(accuracies)\n",
    "best_hp = hp[np.argmax(accuracies)]\n",
    "best_features = feat[np.argmax(accuracies)]\n",
    "    \n",
    "X = pd.DataFrame(data=X, columns=features)\n",
    "X_ext = pd.DataFrame(data=X_ext, columns=features)\n",
    "\n",
    "cols = [col for col in X.columns if col in best_features]\n",
    "\n",
    "X = X[cols]\n",
    "X_ext = X_ext[cols]\n",
    "\n",
    "X = pd.DataFrame.to_numpy(X)\n",
    "X_ext = pd.DataFrame.to_numpy(X_ext)\n",
    "\n",
    "model.set_params(**best_hp)\n",
    "\n",
    "model.fit(X, y)\n",
    "\n",
    "y_predicted_ext = model.predict(X_ext)\n",
    "\n",
    "# calibration curve -----------------------------------------------------------------------------------------\n",
    "fig, ax = plt.subplots()\n",
    "probs_pred = np.array(probs_pred)\n",
    "probs_true = np.array(probs_true)\n",
    "probs_pred = probs_pred.mean(axis=0)\n",
    "probs_true = probs_true.mean(axis=0)\n",
    "plt.plot(probs_pred, probs_true, marker='o', linewidth=1, label='SVC')\n",
    "\n",
    "line = mlines.Line2D([0, 1], [0, 1], color='black', label='Perfectly Calibrated', linestyle = '--')\n",
    "transform = ax.transAxes\n",
    "line.set_transform(transform)\n",
    "ax.add_line(line)\n",
    "fig.suptitle('Calibration plot')\n",
    "ax.set_xlabel('Predicted probability')\n",
    "ax.set_ylabel('True probability in each bin')\n",
    "plt.legend()\n",
    "# plt.show()\n",
    "plt.savefig(f'../../results/svm/combined/svm-cal-curve-external.png')\n",
    "plt.close()\n",
    "# -----------------------------------------------------------------------------------------------------------\n",
    " \n",
    "# ROC -------------------------------------------------------------------------------------------------------\n",
    "tprs = np.array(tprs)\n",
    "mean_tprs = tprs.mean(axis=0)\n",
    "std = tprs.std(axis=0)\n",
    "\n",
    "mean_auc = auc(base_fpr, mean_tprs)\n",
    "std_auc = np.std(aucs)\n",
    "\n",
    "tprs_upper = np.minimum(mean_tprs + std, 1)\n",
    "tprs_lower = mean_tprs - std\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(base_fpr, mean_tprs, 'royalblue', alpha = 0.8, label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),)\n",
    "plt.fill_between(base_fpr, tprs_lower, tprs_upper, color = 'grey', alpha = 0.2)\n",
    "plt.plot([0, 1], [0, 1], linestyle = '--', lw = 2, color = 'r', label = 'Chance', alpha= 0.8)\n",
    "plt.xlim([-0.01, 1.01])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title('Receiver operating characteristic (ROC) curve')\n",
    "#plt.axes().set_aspect('equal', 'datalim')\n",
    "# plt.show()\n",
    "plt.savefig(f'../../results/svm/combined/svm-roc-external.png')\n",
    "plt.close()\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# learning curve ----------------------------------------------------------------------------------------------\n",
    "train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(\n",
    "            model,\n",
    "            X,\n",
    "            y,\n",
    "            cv=cv,\n",
    "            n_jobs=4,\n",
    "            train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "            return_times=True,\n",
    "        )\n",
    "        \n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "fit_times_mean = np.mean(fit_times, axis=1)\n",
    "fit_times_std = np.std(fit_times, axis=1) \n",
    "\n",
    "\n",
    "plot_learning_curve(train_sizes, train_scores_mean, train_scores_std, test_scores_mean, test_scores_std, \n",
    "                    fit_times_mean, fit_times_std, cv=cv, n_jobs=4)\n",
    "plt.savefig(f'../../results/svm/combined/ncv-svm-learning-curve-external.png')\n",
    "plt.close()\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_ext, y_predicted_ext)\n",
    "plotCM(cm, 'x', \"external\", ct_o, 'x' , 0, 0)\n",
    "# print(\"!!!!\", cm_o[0,0], cm_o[0,1], cm_o[1,0], cm_o[1,1])\n",
    "\n",
    "f.write(f'\\n\\nEXTERNAL TEST METRICS')\n",
    "\n",
    "sensitivity = cm[1,1]/(cm[1,1]+cm[1,0]) # TP/(TP+TN)\n",
    "f.write(f'\\nSensitivity: {round(sensitivity*100, 2)}%')\n",
    "specificity = cm[0,0]/(cm[0,0]+cm[0,1]) # TN/(TN+FP)\n",
    "f.write(f'\\nSpecificity: {round(specificity*100, 2)}%')\n",
    "\n",
    "    \n",
    "# metrics\n",
    "accuracy_external = accuracy_score(y_ext, y_predicted_ext)\n",
    "balanced_accuracy_external = balanced_accuracy_score(y_ext, y_predicted_ext)\n",
    "f1_external = f1_score(y_ext, y_predicted_ext)\n",
    "recall_external = recall_score(y_ext, y_predicted_ext)\n",
    "mcc_external = matthews_corrcoef(y_ext, y_predicted_ext)\n",
    "\n",
    "f.write(f'\\n\\nAccuracy: {accuracy_external}')\n",
    "f.write(f'\\nBalanced accuracy: {balanced_accuracy_external}')\n",
    "f.write(f'\\nF1 score: {f1_external}')\n",
    "f.write(f'\\nRecall score: {recall_external}')\n",
    "f.write(f'\\nMCC: {mcc_external}')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b62869",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\n\\nAccuracy: {accuracy_external}')\n",
    "print(f'\\nBalanced accuracy: {balanced_accuracy_external}')\n",
    "print(f'\\nF1 score: {f1_external}')\n",
    "print(f'\\nRecall score: {recall_external}')\n",
    "print(f'\\nMCC: {mcc_external}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c54a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
